# Artificial Intelligence Nanodegree - Term 2

## Program Description

Global innovations in the field of artificial intelligence (AI) are going to redefine virtually every aspect of our lives. Students who master AI skills today will play a critical role in helping determine how this incredible technology impacts our future. This [program](https://www.udacity.com/course/artificial-intelligence-nanodegree--nd889) will introduce you to the broad field of artificial intelligence, and prepare you for a wide variety of opportunities in the AI field.

## Course Syllabus
* [Term 1](https://medium.com/udacity/ai-nanodegree-program-syllabus-term-1-in-depth-80c41297acaf): Foundations of AI  
* [Term 2](https://medium.com/udacity/ai-nanodegree-program-syllabus-term-2-deep-learning-in-depth-d935197b66ec): Deep Learning and Applications  

## Projects: Term 2  

### Recurrent Neural Networks (RNN)
1. [Character-wise Recurrent Neural Networks (RNN)](./intro-to-rnns)  
Built a character-wise RNN trained on Anna Karenina to generate new text based on the text from the book.  

2. [Time Series Prediction and Text Generation](./aind2-rnn)  
Built RNNs that can generate sequences based on input data - with a focus on two applications: used real market data in order to predict future Apple stock prices using an RNN model. The second one will be trained on Sir Arthur Conan Doyle's classic novel Sherlock Holmes and generates wacky sentences based on it that may - or may not - become the next great Sherlock Holmes novel.

### Natural Language Processing
1. [Embedding and Word2Vec](./embeddings/Skip-Gram_word2vec.ipynb)  
Implemented the Word2Vec model to find semantic representations of words for use in natural language processing.

3. [Sentiment Prediction RNN](./sentiment-rnn)  
Implemented a recurrent neural network that can predict if a text sample is positive or negative.  

4. [Character Sequence to Sequence](./seq2seq/seq2seq.ipynb)  
 Implemented a model that takes in a sequence of letters, and outputs a sorted version of that sequence.

5. [Bookworm](./AIND-NLP-Bookworm)  
Built a simple question-answering system using IBM Watson's NLP services.  

7. [Capstone Project: Machine Translation](./aind2-nlp-capstone)  
Built a deep neural network that functions as part of an end-to-end machine translation pipeline. The completed pipeline accepts English text as input and returns the French translation.

### Computer Vision
1. [Convolutional Neural Networks (CNN) Exercises](./aind2-cnn)  

2. [Dog Breed Classifier](./dog-project)  
Built an algorithm to identify canine breed given an image of a dog. If given image of a human, the algorithm identifies a resembling dog breed.

3. [Mimic Me!](./AIND-CV-Mimic)  
Used [Affectiva's](http://www.affectiva.com/) Emotion-as-a-Service API to track faces in a video and identify facial expressions.  

4. [Capstone Project: Facial Keypoints Detection](./AIND-CV-FacialKeypoints)  
Built a deep neural network that can take in any image containing faces and identify the location of each face and their facial keypoints.  

### Generative Adversarial Network (GAN)
1. [GAN on MNIST](./gan_mnist/Intro_to_GANs.ipynb)  
Trained a simple generative adversarial network on the MNIST dataset.  

2. [Deep Convolutional GAN (DCGAN)](./dcgan-svhn/DCGAN.ipynb)  
Implemented a DCGAN to generate new images based on the [Street View House Numbers](http://ufldl.stanford.edu/housenumbers/) (SVHN) dataset.  

3. [Semi-supervised GAN](./semi-supervised/semi-supervised_learning_2.ipynb)  
Trained a semi-supervised GAN to classify the images with a large proportion of the labels dropped.

### Autoencoders  
1. [Simple autoencoder](./autoencoder/Simple_Autoencoder.ipynb)  
Built a simple autoencoder using TensorFlow.

2. [Convolutional Autoencoder](./autoencoder/Convolutional_Autoencoder.ipynb)  
Built models for image compression and denoising, using feed-forward and convolution networks in TensorFlow.

### [Keras Exercises](./aind2-dl)
